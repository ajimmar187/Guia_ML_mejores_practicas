


## **Mejores PrÃ¡cticas en Aprendizaje AutomÃ¡tico**

DespuÃ©s de trabajar en mÃºltiples proyectos que cubren conceptos importantes de aprendizaje automÃ¡tico, tÃ©cnicas y algoritmos ampliamente utilizados, ya tienes una visiÃ³n general del ecosistema del aprendizaje automÃ¡tico, asÃ­ como una experiencia sÃ³lida resolviendo problemas prÃ¡cticos utilizando algoritmos de machine learning y Python. Sin embargo, surgirÃ¡n desafÃ­os cuando empecemos a trabajar en proyectos desde cero en el mundo real. Esta guÃ­a tiene como objetivo prepararnos para ello, con **21 mejores prÃ¡cticas** para seguir a lo largo del flujo de trabajo de una soluciÃ³n de aprendizaje automÃ¡tico.

### En esta guÃ­a cubriremos los siguientes temas:

* Flujo de trabajo de una soluciÃ³n de aprendizaje automÃ¡tico
* Mejores prÃ¡cticas en la etapa de preparaciÃ³n de datos
* Mejores prÃ¡cticas en la generaciÃ³n del conjunto de entrenamiento
* Mejores prÃ¡cticas en el entrenamiento, evaluaciÃ³n y selecciÃ³n del modelo
* Mejores prÃ¡cticas en la etapa de despliegue y monitoreo

---

## **Flujo de trabajo de una soluciÃ³n de aprendizaje automÃ¡tico**

En general, las principales tareas involucradas en resolver un problema de aprendizaje automÃ¡tico pueden resumirse en cuatro Ã¡reas:

* PreparaciÃ³n de datos
* GeneraciÃ³n del conjunto de entrenamiento
* Entrenamiento, evaluaciÃ³n y selecciÃ³n del modelo
* Despliegue y monitoreo

Desde las fuentes de datos hasta el sistema final, una soluciÃ³n de aprendizaje automÃ¡tico sigue bÃ¡sicamente el siguiente paradigma:

*(AquÃ­ se muestra una figura: el ciclo de vida de una soluciÃ³n de aprendizaje automÃ¡tico)*

En las siguientes secciones, aprenderemos sobre las tareas tÃ­picas, desafÃ­os comunes y mejores prÃ¡cticas para cada una de estas cuatro etapas.

---

## **Mejores prÃ¡cticas en la etapa de preparaciÃ³n de datos**

NingÃºn sistema de machine learning puede construirse sin datos. Por lo tanto, la **recolecciÃ³n de datos** debe ser nuestro primer enfoque.

### **Mejor prÃ¡ctica 1 â€“ Comprender completamente el objetivo del proyecto**

Antes de comenzar a recolectar datos, debemos asegurarnos de que el objetivo del proyecto y el problema de negocio estÃ©n completamente entendidos. Esto nos guiarÃ¡ en quÃ© fuentes de datos investigar, y requerirÃ¡ tambiÃ©n un conocimiento y experiencia de dominio suficientes.

Por ejemplo, si el objetivo es predecir los precios futuros de acciones, es crucial recopilar datos sobre el rendimiento histÃ³rico de esas acciones especÃ­ficas, en lugar de datos de mercados no relacionados. Del mismo modo, para optimizar la eficiencia de campaÃ±as de publicidad en lÃ­nea (medida por el ratio de clics), necesitamos recolectar datos detallados sobre los clics: informaciÃ³n de quiÃ©n hizo clic o no, en quÃ© anuncio especÃ­fico, y en quÃ© contexto o pÃ¡gina, en lugar de simplemente contar cuÃ¡ntos anuncios se mostraron.

---

### **Mejor prÃ¡ctica 2 â€“ Recolectar todos los campos relevantes**

Con un objetivo definido, podemos limitar las fuentes de datos potenciales a investigar. La siguiente pregunta es: Â¿es necesario recolectar todos los campos disponibles en una fuente de datos, o basta con un subconjunto?

SerÃ­a ideal saber de antemano quÃ© atributos son clave. Sin embargo, es muy difÃ­cil asegurar que los atributos elegidos por un experto del dominio generen los mejores resultados predictivos. Por ello, se recomienda recolectar **todos los campos relacionados** al proyecto, especialmente si recoleccionar los datos mÃ¡s tarde es costoso o incluso imposible.

Por ejemplo, para predecir precios bursÃ¡tiles, se recolectaron todos los campos disponibles: apertura, mÃ¡ximo, mÃ­nimo, volumen, etc., aunque al principio no sabÃ­amos quÃ© tan Ãºtiles serÃ­an el mÃ¡ximo o mÃ­nimo. En otro caso, si rascamos artÃ­culos en lÃ­nea para clasificaciÃ³n temÃ¡tica, deberÃ­amos guardar la mayor cantidad de informaciÃ³n posible, ya que, si luego descubrimos que un campo omitido (como hipervÃ­nculos) era valioso, puede que ya no sea posible recuperar la fuente.

---

### **Mejor prÃ¡ctica 3 â€“ Mantener la consistencia y normalizaciÃ³n de valores**

En datasets existentes o recolectados, es comÃºn encontrar diferentes valores que representan lo mismo. Ejemplo: en el campo *PaÃ­s*, podemos ver "USA", "U.S.A", "United States"; o en *GÃ©nero*, podemos encontrar "M", "Masculino", "Hombre".

Es **necesario unificar** los valores de los campos. De lo contrario, los algoritmos tratarÃ¡n los mismos significados como distintos. Por ejemplo, podemos mantener solo "M", "F", y "Otro" en el campo de gÃ©nero, reemplazando cualquier otro valor alternativo.

TambiÃ©n es importante asegurar la **consistencia del formato** en un campo. Por ejemplo, en el campo de edad podemos encontrar valores reales como 21, 35, y tambiÃ©n errores como 1990, 1978. En campos de puntuaciÃ³n, puede haber nÃºmeros (1, 2) y palabras ("uno", "dos"). Estos deben transformarse a un formato uniforme.

---

### **Mejor prÃ¡ctica 4 â€“ Tratar los datos faltantes**

En el mundo real, los datasets casi nunca estÃ¡n completamente limpios. Pueden tener valores faltantes o corruptos, como espacios en blanco, *Null*, -1, 999999, *unknown*, etc.

Estos valores no solo aportan informaciÃ³n incompleta, sino que pueden **confundir al modelo**, que no sabe si "unknown" o -1 tienen un significado.

Hay tres estrategias bÃ¡sicas para tratar los valores faltantes:

1. Eliminar muestras que contienen valores faltantes
2. Eliminar campos que tengan algÃºn valor faltante
3. **Imputar valores faltantes**: reemplazarlos por la media, mediana o el valor mÃ¡s frecuente del campo.

Por ejemplo, con este conjunto:
`(30, 100), (20, 50), (35, unknown), (25, 80), (30, 70), (40, 60)`

* Si eliminamos las muestras con faltantes (estrategia 1):
  `(30, 100), (20, 50), (25, 80), (30, 70), (40, 60)`
* Si eliminamos el campo completo con faltantes (estrategia 2):
  Solo queda el primer campo: 30, 20, 35, 25, 30, 40
* Si imputamos el valor faltante con la **media** del campo:
  `(35, 72)` â€” suponiendo que la media es 72
  


Python ofrece herramientas como `SimpleImputer` en Scikit-learn para realizar imputaciÃ³n automÃ¡ticamente.

---

### **Mejor prÃ¡ctica 5 â€“ Almacenamiento de datos a gran escala**

Con el crecimiento exponencial del volumen de datos, muchas veces no podemos almacenar todo en una sola mÃ¡quina local. Por eso, necesitamos recurrir al **almacenamiento en la nube** o sistemas de archivos distribuidos.

Existen dos estrategias principales para escalar el almacenamiento:

* **Escalado vertical (scale-up):** consiste en aumentar la capacidad del sistema actual, por ejemplo, aÃ±adiendo mÃ¡s discos. Es Ãºtil cuando se requiere acceso rÃ¡pido.
* **Escalado horizontal (scale-out):** se incrementa la capacidad aÃ±adiendo nuevos nodos a un clÃºster. Sistemas como **HDFS** (Hadoop Distributed File System) o **Spark** se utilizan para distribuir los datos entre cientos o miles de nodos.

TambiÃ©n existen servicios de almacenamiento en la nube, como:

* **Amazon S3** (AWS)
* **Google Cloud Storage**
* **Microsoft Azure Storage**

AdemÃ¡s del sistema de almacenamiento, se deben considerar las siguientes prÃ¡cticas:

* **Particionado de datos:** dividir en fragmentos pequeÃ±os para distribuir la carga.
* **CompresiÃ³n y codificaciÃ³n:** reducir espacio de almacenamiento y tiempos de recuperaciÃ³n.
* **ReplicaciÃ³n:** duplicar datos en distintos nodos o ubicaciones para tolerancia a fallos.
* **Seguridad y control de acceso:** asegurar que solo usuarios autorizados accedan a los datos.

Una vez que los datos estÃ¡n bien preparados, podemos pasar a la **generaciÃ³n del conjunto de entrenamiento**.

---

## **Mejores prÃ¡cticas en la etapa de generaciÃ³n del conjunto de entrenamiento**

Las tareas tÃ­picas en esta etapa se agrupan en dos categorÃ­as:

1. **Preprocesamiento de datos**
2. **IngenierÃ­a de caracterÃ­sticas (features)**

---

### **Mejor prÃ¡ctica 6 â€“ Identificar variables categÃ³ricas con valores numÃ©ricos**

Las variables categÃ³ricas suelen ser obvias (nivel de riesgo, ocupaciÃ³n, intereses), pero a veces pueden parecer numÃ©ricas, como:

* 1 a 12 (meses del aÃ±o)
* 0 y 1 (falso/verdadero)

**Â¿CÃ³mo diferenciarlas?**
Si implican una relaciÃ³n matemÃ¡tica o de orden, son numÃ©ricas (por ejemplo: calificaciÃ³n de 1 a 5). Si no, son categÃ³ricas (meses, dÃ­as de la semana, etc.).

---

### **Mejor prÃ¡ctica 7 â€“ Decidir si codificar variables categÃ³ricas**

Si una caracterÃ­stica es categÃ³rica, hay que decidir si codificarla o no, dependiendo del algoritmo:

* **NaÃ¯ve Bayes y algoritmos basados en Ã¡rboles** pueden usar variables categÃ³ricas directamente.
* Otros algoritmos (regresiÃ³n, SVM, redes neuronales) **requieren codificaciÃ³n**, como *one-hot encoding* o *label encoding*.

Es clave ver las etapas de **generaciÃ³n de caracterÃ­sticas** y **entrenamiento del modelo** como un conjunto, no como procesos separados.

---

### **Mejor prÃ¡ctica 8 â€“ Decidir si seleccionar caracterÃ­sticas, y cÃ³mo hacerlo**

Seleccionar caracterÃ­sticas puede reducir tiempo de entrenamiento, evitar sobreajuste y mejorar rendimiento.
Ejemplos: regresiÃ³n logÃ­stica con regularizaciÃ³n L1, random forest.

Sin embargo, no siempre mejora la precisiÃ³n, por lo que **es recomendable comparar** resultados con y sin selecciÃ³n usando validaciÃ³n cruzada.

Un ejemplo en Scikit-learn con el dataset de dÃ­gitos escritos a mano muestra que usando solo las 25 caracterÃ­sticas mÃ¡s importantes (de 64), la precisiÃ³n del modelo SVM mejora de 0.90 a 0.95.

---

### **Mejor prÃ¡ctica 9 â€“ Decidir si reducir la dimensionalidad, y cÃ³mo hacerlo**

A diferencia de la selecciÃ³n de caracterÃ­sticas, la reducciÃ³n de dimensionalidad transforma las variables originales a un nuevo espacio (por ejemplo, PCA).

Ventajas:

* Reduce tiempo de entrenamiento
* Disminuye sobreajuste
* Puede mejorar el rendimiento

Al igual que antes, no garantiza mejor desempeÃ±o. Debe evaluarse con validaciÃ³n cruzada. En el mismo dataset de dÃ­gitos, reducir a las 15 componentes principales con PCA tambiÃ©n mejora el rendimiento a 0.95.

---

### **Mejor prÃ¡ctica 10 â€“ Decidir si escalar las caracterÃ­sticas**

Modelos como regresiÃ³n lineal con SGD, SVR y redes neuronales **requieren** que las caracterÃ­sticas estÃ©n estandarizadas (media cero, varianza unitaria).

**Â¿CuÃ¡ndo no es necesario?**

* NaÃ¯ve Bayes y Ã¡rboles de decisiÃ³n no son sensibles a escalas distintas.

**Â¿CuÃ¡ndo sÃ­ es necesario?**

* Algoritmos que utilizan distancias o separaciÃ³n en el espacio (SVC, SVR, KNN, K-means)
* Algoritmos que usan descenso de gradiente (regresiÃ³n/logÃ­stica, redes neuronales)



---

## **Mejor prÃ¡ctica 11 â€“ Realizar ingenierÃ­a de caracterÃ­sticas con conocimiento del dominio**

Si contamos con conocimiento del dominio, podemos crear caracterÃ­sticas especÃ­ficas que se alineen con el negocio y el problema.
Por ejemplo, al predecir precios bursÃ¡tiles, podemos diseÃ±ar caracterÃ­sticas basadas en factores que los inversionistas suelen considerar, como el volumen de transacciones o las variaciones diarias.

TambiÃ©n hay prÃ¡cticas generales aplicables sin importar el dominio. En marketing o anÃ¡lisis de clientes, **el momento del dÃ­a, dÃ­a de la semana o mes** suelen ser seÃ±ales importantes.
Dado un dato con la fecha `2020/09/01` y hora `14:34:21`, podrÃ­amos generar caracterÃ­sticas como: `tarde`, `martes`, `septiembre`.

En comercio minorista, tambiÃ©n es Ãºtil **agrupar informaciÃ³n temporalmente**:
Ejemplo:

* Total de visitas en los Ãºltimos tres meses
* Promedio de productos comprados semanalmente el aÃ±o anterior

Estas son buenas predicciones del comportamiento futuro del cliente.

---

## **Mejor prÃ¡ctica 12 â€“ Realizar ingenierÃ­a de caracterÃ­sticas sin conocimiento del dominio**

Â¿Y si no tenemos conocimiento especÃ­fico? No te preocupes. Hay enfoques **genÃ©ricos** que puedes aplicar:

### **BinarizaciÃ³n y discretizaciÃ³n**

**BinarizaciÃ³n:** transforma una caracterÃ­stica numÃ©rica en binaria con un umbral.
Ejemplo: si el tÃ©rmino â€œpremioâ€ aparece mÃ¡s de una vez en un correo, lo codificamos como 1, si no, como 0.

```python
from sklearn.preprocessing import Binarizer
X = [[4], [1], [3], [0]]
binarizer = Binarizer(threshold=2.9)
X_new = binarizer.fit_transform(X)
# Resultado: [[1], [0], [1], [0]]
```

**DiscretizaciÃ³n:** convierte un nÃºmero en categorÃ­as.
Ejemplo: para el campo edad podrÃ­amos crear grupos:

* 18â€“24
* 25â€“34
* 35â€“54
* 55+

---

### **InteracciÃ³n entre caracterÃ­sticas**

Crear nuevas caracterÃ­sticas combinando otras:

* NumÃ©ricas: suma, producto, etc.

  * Ej: visitas por semana Ã— productos comprados por semana â†’ productos por visita.
* CategÃ³ricas: combinaciÃ³n conjunta

  * Ej: profesiÃ³n e interÃ©s â†’ "ingeniero deportista"

---

### **TransformaciÃ³n polinÃ³mica**

Genera nuevas caracterÃ­sticas mediante potencias e interacciones entre variables.

Ejemplo con Scikit-learn:

```python
from sklearn.preprocessing import PolynomialFeatures

X = [[2, 4], [1, 3], [3, 2], [0, 3]]
poly = PolynomialFeatures(degree=2)
X_new = poly.fit_transform(X)
```

Esto genera:

* 1 (intercepto)
* a, b, aÂ², ab, bÂ²

---

## **Mejor prÃ¡ctica 13 â€“ Documentar cÃ³mo se generÃ³ cada caracterÃ­stica**

Puede parecer trivial, pero muchas veces se nos olvida cÃ³mo se creÃ³ una caracterÃ­stica.
Esto se vuelve importante si el modelo falla y necesitamos regresar a crear nuevas variables o eliminar las que no funcionaron.

**Registrar cada paso** permite modificar, reproducir y mejorar sin perder contexto.

---

## **Mejor prÃ¡ctica 14 â€“ Extraer caracterÃ­sticas de texto**

Hay dos enfoques principales:

### **1. Enfoques tradicionales: TF y TF-IDF**

* **TF (Term Frequency):** cuenta cuÃ¡ntas veces aparece un tÃ©rmino.
* **TF-IDF:** ajusta el conteo penalizando tÃ©rminos comunes y destacando los raros.

Esto se conoce como **Bolsa de Palabras (BoW)**, y no considera el orden de los tÃ©rminos.
Desventaja: genera vectores dispersos, de alta dimensionalidad y sin contexto semÃ¡ntico.

---

### **2. Word Embedding (incrustaciones de palabras)**

A diferencia de TF/TF-IDF, word embedding representa cada palabra como un vector **denso de valores reales**.
Estos vectores capturan **significados y relaciones semÃ¡nticas**.
Ejemplo: los vectores de "clustering" y "grouping" estarÃ¡n cerca si se usan en contextos similares.

#### **Formas de obtener embeddings:**

* **Entrenar con Word2Vec** usando *Skip-gram* o *CBOW*
* **Usar embeddings preentrenados** como:

  * **FastText**
  * **GloVe**
  * **BERT**
  * **GPT**
  * **USE (Universal Sentence Encoder)**

#### **Ejemplo simple con Gensim (Word2Vec):**

```python
from gensim.models import Word2Vec

sentences = [
    ["i", "love", "machine", "learning", "by", "example"],
    ["machine", "learning", "and", "deep", "learning", "are", "fascinating"]
]

model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=0)
vector = model.wv["machine"]
```

---

#### **Embeddings en redes neuronales**

En redes neuronales modernas (especialmente en NLP), se usa una **capa de embedding** que aprende automÃ¡ticamente las representaciones.

**Ejemplo en PyTorch:**

```python
import torch
import torch.nn as nn

input_data = torch.LongTensor([[1, 2, 3, 4], [5, 1, 6, 3]])
embedding_layer = nn.Embedding(10, 3)
embedded_data = embedding_layer(input_data)
```

Esto convierte Ã­ndices de palabras en vectores.

---

## **Mejores prÃ¡cticas en la etapa de entrenamiento, evaluaciÃ³n y selecciÃ³n del modelo**

Al abordar un problema supervisado, muchos se preguntan de inmediato:
**Â¿CuÃ¡l es el mejor algoritmo para clasificar o hacer regresiÃ³n?**
No hay una respuesta universal. No existe una soluciÃ³n mÃ¡gica. No sabrÃ¡s cuÃ¡l algoritmo funciona mejor **hasta probar varios** y ajustar sus parÃ¡metros.

---

### **Mejor prÃ¡ctica 15 â€“ Elegir el(los) algoritmo(s) inicial(es)**

Probar todos los algoritmos y ajustarlos es costoso. En vez de eso, selecciona de **1 a 3 algoritmos candidatos** utilizando estas consideraciones:

* TamaÃ±o del dataset
* Dimensionalidad de los datos
* Â¿Los datos son linealmente separables?
* Â¿Las caracterÃ­sticas son independientes?
* Tolerancia al sesgo/varianza
* Â¿Se necesita aprendizaje en lÃ­nea?

#### **GuÃ­a breve por algoritmo:**

##### **NaÃ¯ve Bayes**

* Ideal si las caracterÃ­sticas son independientes.
* Funciona bien incluso con pocos datos.
* Entrena muy rÃ¡pido.
* Alto sesgo, baja varianza.

##### **RegresiÃ³n logÃ­stica**

* Muy utilizada, ideal cuando los datos son (aproximadamente) linealmente separables.
* Escalable a datasets grandes con descenso de gradiente estocÃ¡stico (SGD).
* Soporta aprendizaje en lÃ­nea.
* Riesgo de sobreajuste mitigado con regularizaciÃ³n L1/L2.

##### **SVM (MÃ¡quinas de Vectores de Soporte)**

* Adaptable a separaciÃ³n lineal o no lineal (mediante kernels).
* Excelente para datos de alta dimensionalidad (ej.: clasificaciÃ³n de textos).
* Precisa, pero computacionalmente exigente.

##### **Ãrboles de decisiÃ³n / Random Forest**

* No importa la separaciÃ³n lineal.
* Acepta variables categÃ³ricas sin codificaciÃ³n.
* Modelo interpretable y explicable.
* Random Forest reduce sobreajuste mediante el ensamblado de Ã¡rboles.

##### **Redes neuronales**

* Muy potentes, especialmente en visiÃ³n por computadora y procesamiento de texto.
* DifÃ­ciles de ajustar (capas, nodos, funciones de activaciÃ³n, etc.).
* Requieren muchos datos.
* Costosas en tiempo de entrenamiento.

---

### **Mejor prÃ¡ctica 16 â€“ Reducir el sobreajuste**

Recapitulando estrategias clave:

* **MÃ¡s datos**: ayuda a evitar que el modelo aprenda ruido.
* **Simplificar el modelo**: evita complejidad innecesaria.
* **ValidaciÃ³n cruzada**: buena prÃ¡ctica estÃ¡ndar.
* **RegularizaciÃ³n**: penaliza la complejidad (L1, L2).
* **Early stopping**: detener el entrenamiento cuando el rendimiento en validaciÃ³n se degrada.
* **Dropout** (en redes): desconecta aleatoriamente neuronas durante el entrenamiento.
* **SelecciÃ³n de caracterÃ­sticas**: eliminar atributos irrelevantes.
* **Ensamblado**: combinar modelos simples (bagging, boosting).

---

### **Mejor prÃ¡ctica 17 â€“ Diagnosticar sobreajuste y subajuste**

Usamos **curvas de aprendizaje** para analizar bias y varianza. Se comparan los errores en entrenamiento vs validaciÃ³n con el nÃºmero de muestras.

* **Sobreajuste**: alto rendimiento en entrenamiento, bajo en validaciÃ³n.
* **Subajuste**: ambos rendimientos bajos.
* **Ideal**: las curvas convergen con rendimiento alto.

Scikit-learn proporciona el mÃ³dulo `learning_curve` para visualizar estos grÃ¡ficos y diagnosticar problemas.

---

### **Mejor prÃ¡ctica 18 â€“ Modelar datasets a gran escala**

Trabajar con grandes volÃºmenes requiere estrategia:

#### **Consejos clave:**

* **Empieza con un subconjunto pequeÃ±o**: para experimentar rÃ¡pidamente.
* **Usa algoritmos escalables**: regresiÃ³n logÃ­stica, SVM lineal, SGD.
* **ComputaciÃ³n distribuida**: frameworks como Apache Spark.
* **ReducciÃ³n de dimensionalidad**: PCA, t-SNE si es necesario.
* **ParalelizaciÃ³n**: usar mÃºltiples GPUs o nodos.
* **AdministraciÃ³n de memoria**: carga por lotes, liberaciÃ³n eficiente.
* **Bibliotecas optimizadas**: como TensorFlow, PyTorch, XGBoost.
* **Aprendizaje incremental**: para datos en streaming o que llegan progresivamente.

> âš ï¸ Â¡No olvides guardar el modelo entrenado! Entrenar con datos grandes toma tiempo y recursos.

---

## **Mejores prÃ¡cticas en la etapa de despliegue y monitoreo**

DespuÃ©s de preparar los datos, generar el conjunto de entrenamiento y entrenar el modelo, llega el momento de **desplegar el sistema**. AquÃ­ nos aseguramos de que los modelos funcionen bien en producciÃ³n, se actualicen si es necesario y sigan ofreciendo valor real.

---

### **Mejor prÃ¡ctica 19 â€“ Guardar, cargar y reutilizar modelos**

Al desplegar un modelo, los nuevos datos deben pasar por el **mismo proceso de preprocesamiento** que se usÃ³ en el entrenamiento: escalado, ingenierÃ­a de caracterÃ­sticas, selecciÃ³n, etc.

Por eso, **no se debe reentrenar todo el pipeline desde cero cada vez**. En cambio, se deben guardar:

* El modelo de preprocesamiento (escaladores, transformadores)
* El modelo entrenado

Y luego cargarlos cuando se necesiten para hacer predicciones.

#### **Guardar con `joblib` (Scikit-learn)**

```python
from joblib import dump, load

# Guardar el escalador
dump(scaler, "scaler.joblib")

# Guardar el modelo
dump(regressor, "regressor.joblib")
```

Luego, en producciÃ³n:

```python
# Cargar los objetos
scaler = load("scaler.joblib")
regressor = load("regressor.joblib")

# Preprocesar y predecir
X_scaled = scaler.transform(X_new)
predicciones = regressor.predict(X_scaled)
```

Joblib es mÃ¡s eficiente que pickle para objetos de NumPy y modelos de machine learning, especialmente con datasets grandes, y ofrece mejor compresiÃ³n y rendimiento.

---

#### **Guardar modelos en TensorFlow**

```python
# Guardar modelo completo
model.save('./modelo_tf')

# Cargar modelo
nuevo_modelo = tf.keras.models.load_model('./modelo_tf')
```

---

#### **Guardar modelos en PyTorch**

```python
# Guardar modelo completo
torch.save(model, './modelo.pth')

# Cargar modelo
nuevo_modelo = torch.load('./modelo.pth')
```

Esto guarda arquitectura, pesos y configuraciÃ³n del entrenamiento.

---

### **Mejor prÃ¡ctica 20 â€“ Monitorear el rendimiento del modelo**

Una vez desplegado el modelo, **debe ser monitoreado continuamente** para asegurarse de que siga funcionando bien. Algunos consejos:

* **Define mÃ©tricas claras**: precisiÃ³n, F1, AUC-ROC, RÂ², error cuadrÃ¡tico medio, etc.
* **Compara contra un modelo base** (baseline): Ãºtil como referencia.
* **Curvas de aprendizaje**: visualizan si hay sobreajuste o subajuste.

Ejemplo en Scikit-learn:

```python
from sklearn.metrics import r2_score
print(f'Chequeo del modelo, R^2: {r2_score(y_nuevo, predicciones):.3f}')
```

AdemÃ¡s, deberÃ­as registrar (loggear) estas mÃ©tricas y **activar alertas** si el rendimiento baja.

---

### **Mejor prÃ¡ctica 21 â€“ Actualizar los modelos regularmente**

Con el tiempo, los datos pueden cambiar (fenÃ³meno conocido como *data drift*). Si el rendimiento se deteriora:

* **Monitorea constantemente**: si las mÃ©tricas bajan, es momento de actuar.
* **Actualizaciones programadas**: segÃºn frecuencia de cambios en los datos.
* **Aprendizaje en lÃ­nea (online learning)**: para modelos como regresiÃ³n con SGD o NaÃ¯ve Bayes, que se pueden actualizar sin reentrenar.
* **Control de versiones**: tanto de modelos como de datasets.
* **AuditorÃ­as regulares**: revisa si las mÃ©tricas, objetivos de negocio o datos han cambiado.

> ðŸ“Œ Monitorear es un proceso continuo, no algo que se hace una sola vez.

---

## **Resumen**

Esta guÃ­a te prepara para resolver problemas reales de machine learning. Repasamos el flujo de trabajo tÃ­pico:

1. PreparaciÃ³n de datos
2. GeneraciÃ³n del conjunto de entrenamiento
3. Entrenamiento, evaluaciÃ³n y selecciÃ³n de modelos
4. Despliegue y monitoreo

Para cada etapa, detallamos tareas, desafÃ­os comunes y **21 mejores prÃ¡cticas**.

> âœ… **La mejor prÃ¡ctica de todas es practicar.**
> Empieza un proyecto real y aplica lo que has aprendido.